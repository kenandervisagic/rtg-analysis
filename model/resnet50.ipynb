{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pneumonia Detection: Transfer Learning with ResNet50\n",
    "## Thesis Section: Improved Model\n",
    "This notebook uses transfer learning with ResNet50, pre-trained on ImageNet, to detect pneumonia from chest X-ray images using the Kaggle Chest X-Ray Pneumonia dataset ([Kaggle link](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia)). By leveraging pre-trained features and fine-tuning, this approach aims to achieve higher accuracy (expected 90-95%) than the CNN from scratch, making it suitable for the web service backend.\n",
    "\n",
    "The dataset is the same as in the CNN notebook, with class imbalance addressed via class weights. We use more aggressive data augmentation and fine-tune the last 50 layers of ResNet50 to adapt to the specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Kaggle API to download dataset\n",
    "!pip install -q kaggle\n",
    "\n",
    "# Upload kaggle.json file\n",
    "from google.colab import files\n",
    "files.upload()  # Upload kaggle.json\n",
    "\n",
    "# Set up Kaggle directory and permissions\n",
    "!mkdir -p ~/.kaggle\n",
    "!mv kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Download and unzip the chest X-ray pneumonia dataset\n",
    "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
    "!unzip -q chest-xray-pneumonia.zip -d chest_xray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "We verify the dataset structure, noting the imbalance in the training set (1,341 normal vs. 3,875 pneumonia images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define dataset paths\n",
    "base_dir = 'chest_xray/chest_xray'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "val_dir = os.path.join(base_dir, 'val')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# Count images\n",
    "train_normal = len(os.listdir(os.path.join(train_dir, 'NORMAL')))\n",
    "train_pneumonia = len(os.listdir(os.path.join(train_dir, 'PNEUMONIA')))\n",
    "print(f'Training set: {train_normal} normal, {train_pneumonia} pneumonia images')\n",
    "print(f'Validation set: {len(os.listdir(os.path.join(val_dir, \"NORMAL\")))} normal, {len(os.listdir(os.path.join(val_dir, \"PNEUMONIA\")))} pneumonia images')\n",
    "print(f'Test set: {len(os.listdir(os.path.join(test_dir, \"NORMAL\")))} normal, {len(os.listdir(os.path.join(test_dir, \"PNEUMONIA\")))} pneumonia images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Images are resized to 224x224 (standard for ResNet50) and augmented with rotation, zoom, shifts, and brightness adjustments to improve robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Parameters\n",
    "img_height, img_width = 224, 224\n",
    "batch_size = 32\n",
    "\n",
    "# Data generators with augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2]\n",
    ")\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load data\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "val_data = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "test_data = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Weights\n",
    "Class weights are computed to balance the training process, giving higher weight to the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Compute class weights\n",
    "labels = train_data.classes\n",
    "weights = class_weight.compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights = dict(enumerate(weights))\n",
    "print('Class weights:', class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "We use ResNet50 pre-trained on ImageNet, freezing its layers initially and adding a custom classification head with GlobalAveragePooling2D, a dense layer, dropout, and a sigmoid output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load ResNet50\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add classification head\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Training\n",
    "We train the classification head for 10 epochs, keeping the base model frozen, using early stopping and model checkpointing to save the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train classification head\n",
    "epochs_initial = 10\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=epochs_initial,\n",
    "    validation_data=val_data,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "We unfreeze the last 50 layers of ResNet50 and fine-tune with a lower learning rate (1e-5) for 10 epochs to adapt the model to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune last 50 layers\n",
    "for layer in base_model.layers[-50:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(optimizer=Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Continue training\n",
    "epochs_finetune = 10\n",
    "history_finetune = model.fit(\n",
    "    train_data,\n",
    "    epochs=epochs_finetune,\n",
    "    validation_data=val_data,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Visualization\n",
    "We evaluate the fine-tuned model on the test set and visualize combined training history and a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Plot combined training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'] + history_finetune.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'] + history_finetune.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'] + history_finetune.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'] + history_finetune.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc = model.evaluate(test_data)\n",
    "print(f'Test Accuracy: {test_acc:.3f}')\n",
    "\n",
    "# Generate confusion matrix\n",
    "y_pred = (model.predict(test_data) > 0.5).astype(int)\n",
    "y_true = test_data.classes\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_true, y_pred, target_names=['Normal', 'Pneumonia']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Saving\n",
    "The fine-tuned model is saved for backend integration and downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and download model\n",
    "model.save('resnet50_pneumonia.h5')\n",
    "files.download('resnet50_pneumonia.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "The ResNet50 model, with transfer learning and fine-tuning, achieves higher accuracy (expected 90-95%) than the CNN from scratch, due to its pre-trained features and adaptation to the dataset. This model is suitable for the web service backend. The small validation set remains a limitation, but the test set performance confirms improved reliability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
