{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQJWDdYi4MSp"
      },
      "source": [
        "# Pneumonia Detection: Transfer Learning with ResNet50\n",
        "## Thesis Section: Improved Model\n",
        "This notebook uses transfer learning with ResNet50, pre-trained on ImageNet, to detect pneumonia from chest X-ray images using the Kaggle Chest X-Ray Pneumonia dataset ([Kaggle link](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia)). By leveraging pre-trained features and fine-tuning, this approach aims to achieve higher accuracy (target 90-95%) than the CNN from scratch, making it suitable for the web service backend.\n",
        "\n",
        "The dataset is merged (train + val) and split into 80/10/10 to address the small validation set (8 normal, 8 pneumonia). Class imbalance is handled with adjusted weights, and aggressive augmentation with fine-tuning of more layers is applied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "g9vvqPxb4MSq",
        "outputId": "d7f58eee-d6f3-4c3f-d989-851b70599f42"
      },
      "outputs": [],
      "source": [
        "# Install Kaggle API to download dataset\n",
        "!pip install -q kaggle\n",
        "\n",
        "# Upload kaggle.json file\n",
        "from google.colab import files\n",
        "files.upload()  # Upload kaggle.json\n",
        "\n",
        "# Set up Kaggle directory and permissions\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download and unzip the chest X-ray pneumonia dataset\n",
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
        "!unzip -q chest-xray-pneumonia.zip -d chest_xray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfnzbgtH4MSr"
      },
      "source": [
        "## Data Exploration and Splitting\n",
        "Merge train and val sets, then create a new 80/10/10 split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YVoeTYM_4MSs",
        "outputId": "17321a77-e4be-4868-c812-77df81cbc523"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define base directory\n",
        "base_dir = 'chest_xray/chest_xray'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'val')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "# Collect all file paths\n",
        "normal_files = glob.glob(os.path.join(train_dir, 'NORMAL', '*.jpeg')) + glob.glob(os.path.join(val_dir, 'NORMAL', '*.jpeg'))\n",
        "pneumonia_files = glob.glob(os.path.join(train_dir, 'PNEUMONIA', '*.jpeg')) + glob.glob(os.path.join(val_dir, 'PNEUMONIA', '*.jpeg'))\n",
        "all_files = normal_files + pneumonia_files\n",
        "labels = [0] * len(normal_files) + [1] * len(pneumonia_files)\n",
        "\n",
        "# First split: 90% (train+val) and 10% (test)\n",
        "train_val_files, test_files, train_val_labels, test_labels = train_test_split(all_files, labels, test_size=0.1, stratify=labels, random_state=42)\n",
        "\n",
        "# Second split: 80% train, 20% val from train_val\n",
        "train_files, val_files, train_labels, val_labels = train_test_split(train_val_files, train_val_labels, test_size=0.2222, stratify=train_val_labels, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooEeKJdl4MSs"
      },
      "source": [
        "## Data Preprocessing\n",
        "Images are resized to 224x224, with enhanced augmentation for robustness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "iWvvIPQ04MSs",
        "outputId": "54a29a94-e750-4c07-a084-a3557af493da"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Parameters\n",
        "img_height, img_width = 224, 224\n",
        "batch_size = 128\n",
        "\n",
        "# Data generators with enhanced augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.2,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    shear_range=0.2,\n",
        "    brightness_range=[0.8, 1.2]\n",
        ")\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load data with new split (using directories or DataFrame approach)\n",
        "import pandas as pd\n",
        "train_df = pd.DataFrame({'filename': train_files, 'class': train_labels})\n",
        "val_df = pd.DataFrame({'filename': val_files, 'class': val_labels})\n",
        "test_df = pd.DataFrame({'filename': test_files, 'class': test_labels})\n",
        "\n",
        "train_data = train_datagen.flow_from_dataframe(\n",
        "    train_df,\n",
        "    x_col='filename',\n",
        "    y_col='class',\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "val_data = val_datagen.flow_from_dataframe(\n",
        "    val_df,\n",
        "    x_col='filename',\n",
        "    y_col='class',\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "test_data = test_datagen.flow_from_dataframe(\n",
        "    test_df,\n",
        "    x_col='filename',\n",
        "    y_col='class',\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtwMLKNz4MSs"
      },
      "source": [
        "## Class Weights\n",
        "Adjusted class weights to balance the training process, increasing weight for pneumonia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SEF33byh4MSs",
        "outputId": "63081bf5-d191-43c0-cd2f-37f0407caa27"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Compute adjusted class weights\n",
        "labels = train_labels\n",
        "weights = class_weight.compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
        "class_weights = {0: weights[0], 1: 0.8}  # Manually adjust pneumonia weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPVsVtk54MSs"
      },
      "source": [
        "## Model Architecture\n",
        "ResNet50 with more layers fine-tuned and learning rate scheduling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aGzLhq6C4MSs",
        "outputId": "b773de12-43b2-4c67-fce4-10e4c0cb1eec"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "# Load ResNet50 base model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
        "\n",
        "# Unfreeze last 100 layers for fine-tuning\n",
        "for layer in base_model.layers[:-100]:\n",
        "    layer.trainable = False\n",
        "for layer in base_model.layers[-100:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Add custom head\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPVsVtk54MSs"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "aGzLhq6C4MSs",
        "outputId": "b773de12-43b2-4c67-fce4-10e4c0cb1eec"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    epochs=20,\n",
        "    validation_data=val_data,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[reduce_lr, early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPVsVtk54MSs"
      },
      "source": [
        "## Evaluation and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "aGzLhq6C4MSs",
        "outputId": "b773de12-43b2-4c67-fce4-10e4c0cb1eec"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Plot combined training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc = model.evaluate(test_data)\n",
        "print(f'Test Accuracy: {test_acc:.3f}')\n",
        "\n",
        "# Generate confusion matrix\n",
        "y_pred = (model.predict(test_data) > 0.4).astype(int)  # Adjusted threshold\n",
        "y_true = test_data.classes\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_true, y_pred, target_names=['Normal', 'Pneumonia']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2EtO4nb4MSu"
      },
      "source": [
        "## Model Saving\n",
        "The fine-tuned model is saved for backend integration and downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "uUyRwU594MSu",
        "outputId": "714cd3aa-aad0-408c-b06c-3f48b1d52c25"
      },
      "outputs": [],
      "source": [
        "# Save and download model\n",
        "model.save('resnet50_pneumonia_optimized.keras')\n",
        "files.download('resnet50_pneumonia_optimized.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfSAUHCm4MSu"
      },
      "source": [
        "## Discussion\n",
        "The optimized ResNet50 model, with a larger validation set, adjusted weights, and enhanced fine-tuning, targets 90-95% accuracy. The new split improves metric stability, while increased pneumonia weight boosts recall. Limitations include potential over-augmentation, which should be monitored."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}