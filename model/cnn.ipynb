{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pneumonia Detection: CNN from Scratch\n",
    "## Thesis Section: Baseline Model\n",
    "This notebook trains a convolutional neural network (CNN) from scratch to detect pneumonia from chest X-ray images using the Kaggle Chest X-Ray Pneumonia dataset ([Kaggle link](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia)). The goal is to establish a baseline model for comparison with transfer learning approaches in the thesis, demonstrating that training from scratch yields moderate accuracy due to limited model complexity and dataset size.\n",
    "\n",
    "The dataset contains 5,863 images (train: 1,341 normal, 3,875 pneumonia; validation: 16 images; test: 624 images). Due to class imbalance, we apply class weights during training. Data augmentation is used to improve generalization, and the model is evaluated using accuracy, precision, recall, F1-score, and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Kaggle API to download dataset\n",
    "!pip install -q kaggle\n",
    "\n",
    "# Upload kaggle.json file (from your Kaggle account)\n",
    "from google.colab import files\n",
    "files.upload()  # Upload kaggle.json\n",
    "\n",
    "# Set up Kaggle directory and permissions\n",
    "!mkdir -p ~/.kaggle\n",
    "!mv kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Download and unzip the chest X-ray pneumonia dataset\n",
    "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
    "!unzip -q chest-xray-pneumonia.zip -d chest_xray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "We explore the dataset to understand its structure and class distribution. The training set is imbalanced, with significantly more pneumonia images than normal ones, necessitating class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define dataset paths\n",
    "base_dir = 'chest_xray/chest_xray'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "val_dir = os.path.join(base_dir, 'val')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# Count images in each class\n",
    "train_normal = len(os.listdir(os.path.join(train_dir, 'NORMAL')))\n",
    "train_pneumonia = len(os.listdir(os.path.join(train_dir, 'PNEUMONIA')))\n",
    "print(f'Training set: {train_normal} normal, {train_pneumonia} pneumonia images')\n",
    "print(f'Validation set: {len(os.listdir(os.path.join(val_dir, \"NORMAL\")))} normal, {len(os.listdir(os.path.join(val_dir, \"PNEUMONIA\")))} pneumonia images')\n",
    "print(f'Test set: {len(os.listdir(os.path.join(test_dir, \"NORMAL\")))} normal, {len(os.listdir(os.path.join(test_dir, \"PNEUMONIA\")))} pneumonia images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "We use `ImageDataGenerator` to preprocess images by rescaling pixel values to [0,1] and applying augmentation (rotation, zoom, horizontal flip) to enhance model generalization. Images are resized to 150x150 pixels to reduce computational load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Parameters\n",
    "img_height, img_width = 150, 150\n",
    "batch_size = 32\n",
    "\n",
    "# Data generators with augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load data\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "val_data = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "test_data = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Weights\n",
    "Due to the imbalanced dataset, we compute class weights to assign higher importance to the minority class (normal images) during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Compute class weights\n",
    "labels = train_data.classes\n",
    "weights = class_weight.compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights = dict(enumerate(weights))\n",
    "print('Class weights:', class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "We define a simple CNN with three convolutional layers (32, 64, 128 filters), max-pooling, a flatten layer, a dense layer with dropout (0.5) for regularization, and a sigmoid output for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "The model is trained for 20 epochs with class weights to address imbalance. Validation performance is monitored, though the small validation set (16 images) may lead to noisy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "epochs = 20\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_data,\n",
    "    class_weight=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Visualization\n",
    "We evaluate the model on the test set and visualize training/validation accuracy and loss, as well as a confusion matrix to assess performance across classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc = model.evaluate(test_data)\n",
    "print(f'Test Accuracy: {test_acc:.3f}')\n",
    "\n",
    "# Generate confusion matrix\n",
    "y_pred = (model.predict(test_data) > 0.5).astype(int)\n",
    "y_true = test_data.classes\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_true, y_pred, target_names=['Normal', 'Pneumonia']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Saving\n",
    "The trained model is saved for integration into the web service backend and downloaded for local use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and download model\n",
    "model.save('pneumonia_cnn.h5')\n",
    "files.download('pneumonia_cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "The CNN from scratch achieves moderate accuracy (expected 80-85%) due to limited model depth and dataset size. The small validation set may cause unreliable validation metrics. This baseline highlights the need for a more advanced approach, such as transfer learning, to improve accuracy for the web service."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
